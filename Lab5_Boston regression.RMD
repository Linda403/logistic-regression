---
title: "Lab 5. Boston housing case"
author: "DSO 530"
date: "10/6/2021"
output:
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multiple Linear Regression Model

```{r}
library(MASS)
library(ISLR2)
```

The `ISLR2` library contains the `Boston`  data set, which
records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).
To find out more about the data set, we can type `?Boston`.


@. Let's build the multiple linear regression model. Use the following predictor variables: `lstat`, `dis`, `rm`, `nox`, `crim`. Check the scatterplots and comment on the patterns. Suggest variable transformations based on your visual inspection.

```{r}
names(Boston)
?Boston

pairs(Boston[,-c(2,3,4,7,9,10,11)])
#-c -> emit unwanted columns
#y-axis:crim/ x-axis: nox, etc.?

attach(Boston)
plot(lstat, medv)
#negative correlation, close to linear
plot(dis,medv)
#pretty spread out, no clear correlation
plot(rm,medv)
#positive linear correlation
plot(nox,medv)
#no correlation
plot(crim, medv)
#no correlation
```
@. Build the multiple linear regression model to predict the median value of owner-occupied homes `medv` using the results of the previous step (use the transformed variables if needed).

```{r}
#summary(data)
m1 <- lm(medv~log(lstat)+log(dis)+rm+nox+exp(crim), data = Boston)
m1
summary(m1)
#show coefficients, the higher the better?
```

Suggested transformations: in()

@. Perform the test for the regression effect. Is the effect significant?
```{r}
#regression effect means the variables are predictive?
#F-test(last line in the summary command)
summary(m1)
```
Yes, it is significant. The p-value of F-test is lower than 0.05, which means that at least one of the variables is effective.

@. If yes, what are the significant predictor variables? Run the appropriate test and comment on your results.

Based on the t-test, only log(lstat) is lower than 0.05. log(lstat) is the significant predictor variable

@. Is there multicollinearity problem? Compute the VIF, explore the scatterplots, and the correlation matrix. Comment on your findings. Would you suggest removing variables if any?
```{r}
library(car)
vif(m1)
#vif -> the closer to one the better, no more than 10 is fine
#I wanted to remove "crim" but the vif of it is the most close to one?
```
Only exp(crim) is close to 1. But none of the variables are over 10, so none of the variables need to be removed.

@. Are the other assumptions met? Run the appropriate diagnostics.
```{r}
cor(log(lstat), medv)
cor(lstat, medv)
cor(log(dis), medv)
cor(rm, medv)
cor(nox, medv)
cor(exp(crim), medv)
```

@. Re-run your analysis implementing the findings in the previous steps. Did the model improve. Include your comments.
```{r}
data.lm <- lm(medv~log(lstat)+log(dis)+rm+nox+exp(crim), data = Boston)
data.lm

summary(data.lm)

plot(data.lm, which = 1:3)

abline(data.lm, col="red")

acf(data.lm$residuals)

summary(data.lm)$adj.r.squared
```
This new model seem to improve. The r-square is 0.74, which is pretty high. The rediduals check seem to improved as well.




